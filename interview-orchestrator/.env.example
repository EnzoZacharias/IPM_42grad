# Lokales LLM-Konfiguration
# ===========================

# URL zum lokalen LLM-Server
# Für Ollama (Standard):
LOCAL_LLM_URL=http://localhost:11434

# Für LM Studio:
# LOCAL_LLM_URL=http://localhost:1234

# Für vLLM oder andere OpenAI-kompatible Server:
# LOCAL_LLM_URL=http://localhost:8000

# Modellname (muss auf dem lokalen Server verfügbar sein)
# Für Ollama: Verwende den Namen, mit dem du das Modell gezogen hast
LOCAL_LLM_MODEL=mistral-small

# Beispiele für andere Modelle:
# LOCAL_LLM_MODEL=mistral:7b
# LOCAL_LLM_MODEL=mistral-small:latest
# LOCAL_LLM_MODEL=mistral:instruct
